{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262759f3",
   "metadata": {},
   "source": [
    "# cjm-media-plugin-system\n",
    "\n",
    "> Defines standardized interfaces and data structures for media analysis (VAD, Scene Detection) and processing (FFmpeg, Conversion) plugins within the cjm-plugin-system ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cf2c7",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "```bash\n",
    "pip install cjm_media_plugin_system\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3f4a2",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "```\n",
    "nbs/\n",
    "├── analysis_interface.ipynb   # Domain-specific plugin interface for media analysis (read-only / signal extraction)\n",
    "├── core.ipynb                 # DTOs for media analysis and processing with FileBackedDTO support for zero-copy transfer\n",
    "├── processing_interface.ipynb # Domain-specific plugin interface for media processing (write / file manipulation)\n",
    "└── storage.ipynb              # Standardized SQLite storage for media analysis and processing results with content hashing\n",
    "```\n",
    "\n",
    "Total: 4 notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185d4e4",
   "metadata": {},
   "source": [
    "## Module Dependencies\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    analysis_interface[analysis_interface<br/>Media Analysis Plugin Interface]\n",
    "    core[core<br/>Core Data Structures]\n",
    "    processing_interface[processing_interface<br/>Media Processing Plugin Interface]\n",
    "    storage[storage<br/>Media Storage]\n",
    "\n",
    "    analysis_interface --> core\n",
    "    processing_interface --> core\n",
    "```\n",
    "\n",
    "*2 cross-module dependencies detected*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92debc6",
   "metadata": {},
   "source": [
    "## CLI Reference\n",
    "\n",
    "No CLI commands found in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8dcb7",
   "metadata": {},
   "source": [
    "## Module Overview\n",
    "\n",
    "Detailed documentation for each module in the project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2bc3d",
   "metadata": {},
   "source": [
    "### Media Analysis Plugin Interface (`analysis_interface.ipynb`)\n",
    "> Domain-specific plugin interface for media analysis (read-only / signal extraction)\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_media_plugin_system.analysis_interface import (\n",
    "    MediaAnalysisPlugin\n",
    ")\n",
    "```\n",
    "#### Classes\n",
    "\n",
    "```python\n",
    "class MediaAnalysisPlugin(PluginInterface):\n",
    "    \"\"\"\n",
    "    Abstract base class for plugins that analyze media files.\n",
    "    \n",
    "    Analysis plugins perform read-only operations that extract temporal segments\n",
    "    from media files (VAD, scene detection, beat detection, etc.).\n",
    "    \"\"\"\n",
    "    \n",
    "    def execute(\n",
    "            self,\n",
    "            media_path: Union[str, Path],  # Path to media file to analyze\n",
    "            **kwargs\n",
    "        ) -> MediaAnalysisResult:  # Analysis result with detected TimeRanges\n",
    "        \"Analyze the media file and return detected temporal segments.\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22164d",
   "metadata": {},
   "source": [
    "### Core Data Structures (`core.ipynb`)\n",
    "> DTOs for media analysis and processing with FileBackedDTO support for zero-copy transfer\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_media_plugin_system.core import (\n",
    "    TimeRange,\n",
    "    MediaMetadata,\n",
    "    MediaAnalysisResult\n",
    ")\n",
    "```\n",
    "#### Classes\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TimeRange:\n",
    "    \"Represents a temporal segment within a media file.\"\n",
    "    \n",
    "    start: float  # Start time in seconds\n",
    "    end: float  # End time in seconds\n",
    "    label: str = 'segment'  # Segment type (e.g., 'speech', 'silence', 'scene')\n",
    "    confidence: Optional[float]  # Detection confidence (0.0 to 1.0)\n",
    "    payload: Dict[str, Any] = field(...)  # Extra data (e.g., speaker embedding)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:  # Serialized representation\n",
    "        \"Convert to dictionary for JSON serialization.\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class MediaMetadata:\n",
    "    \"Container for media file metadata.\"\n",
    "    \n",
    "    path: str  # File path\n",
    "    duration: float  # Duration in seconds\n",
    "    format: str  # Container format (e.g., 'mp4', 'mkv')\n",
    "    size_bytes: int  # File size in bytes\n",
    "    video_streams: List[Dict[str, Any]] = field(...)  # Video stream info\n",
    "    audio_streams: List[Dict[str, Any]] = field(...)  # Audio stream info\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:  # Serialized representation\n",
    "        \"Convert to dictionary for JSON serialization.\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class MediaAnalysisResult:\n",
    "    \"Standard output for media analysis plugins.\"\n",
    "    \n",
    "    ranges: List[TimeRange]  # Detected temporal segments\n",
    "    metadata: Dict[str, Any] = field(...)  # Global analysis stats\n",
    "    \n",
    "    def to_temp_file(self) -> str:  # Absolute path to temporary JSON file\n",
    "            \"\"\"Save results to a temp JSON file for zero-copy transfer.\"\"\"\n",
    "            tmp = tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode='w')\n",
    "            \n",
    "            data = {\n",
    "                \"ranges\": [r.to_dict() for r in self.ranges],\n",
    "        \"Save results to a temp JSON file for zero-copy transfer.\"\n",
    "    \n",
    "    def from_file(\n",
    "            cls,\n",
    "            filepath: str  # Path to JSON file\n",
    "        ) -> \"MediaAnalysisResult\":  # Loaded result instance\n",
    "        \"Load results from a JSON file.\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f130fb",
   "metadata": {},
   "source": [
    "### Media Processing Plugin Interface (`processing_interface.ipynb`)\n",
    "> Domain-specific plugin interface for media processing (write / file manipulation)\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_media_plugin_system.processing_interface import (\n",
    "    MediaProcessingPlugin\n",
    ")\n",
    "```\n",
    "#### Classes\n",
    "\n",
    "```python\n",
    "class MediaProcessingPlugin(PluginInterface):\n",
    "    \"\"\"\n",
    "    Abstract base class for plugins that modify, convert, or extract media.\n",
    "    \n",
    "    Processing plugins perform write operations that produce new files\n",
    "    (format conversion, segment extraction, re-encoding, etc.).\n",
    "    \"\"\"\n",
    "    \n",
    "    def execute(\n",
    "            self,\n",
    "            action: str = \"get_info\",  # Operation: 'get_info', 'convert', 'extract_segment'\n",
    "            **kwargs\n",
    "        ) -> Dict[str, Any]:  # JSON-serializable result (usually containing 'output_path')\n",
    "        \"Execute a media processing operation.\"\n",
    "    \n",
    "    def get_info(\n",
    "            self,\n",
    "            file_path: Union[str, Path]  # Path to media file\n",
    "        ) -> MediaMetadata:  # File metadata (duration, codec, streams)\n",
    "        \"Get metadata for a media file.\"\n",
    "    \n",
    "    def convert(\n",
    "            self,\n",
    "            input_path: Union[str, Path],  # Source file path\n",
    "            output_format: str,            # Target format (e.g., 'mp4', 'wav')\n",
    "            **kwargs\n",
    "        ) -> str:  # Path to converted file\n",
    "        \"Convert media to a different format.\"\n",
    "    \n",
    "    def extract_segment(\n",
    "            self,\n",
    "            input_path: Union[str, Path],      # Source file path\n",
    "            start: float,                       # Start time in seconds\n",
    "            end: float,                         # End time in seconds\n",
    "            output_path: Optional[str] = None   # Custom output path (auto-generated if None)\n",
    "        ) -> str:  # Path to extracted segment file\n",
    "        \"Extract a temporal segment from a media file.\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550d087",
   "metadata": {},
   "source": [
    "### Media Storage (`storage.ipynb`)\n",
    "> Standardized SQLite storage for media analysis and processing results with content hashing\n",
    "\n",
    "#### Import\n",
    "\n",
    "```python\n",
    "from cjm_media_plugin_system.storage import (\n",
    "    MediaAnalysisRow,\n",
    "    MediaAnalysisStorage,\n",
    "    MediaProcessingRow,\n",
    "    MediaProcessingStorage\n",
    ")\n",
    "```\n",
    "#### Classes\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class MediaAnalysisRow:\n",
    "    \"A single row from the analysis_jobs table.\"\n",
    "    \n",
    "    file_path: str  # Path to the analyzed media file\n",
    "    file_hash: str  # Hash of source file in \"algo:hexdigest\" format\n",
    "    config_hash: str  # Hash of the analysis config used\n",
    "    ranges: Optional[List[Dict[str, Any]]]  # Detected temporal segments\n",
    "    metadata: Optional[Dict[str, Any]]  # Analysis metadata\n",
    "    created_at: Optional[float]  # Unix timestamp\n",
    "```\n",
    "\n",
    "```python\n",
    "class MediaAnalysisStorage:\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str  # Absolute path to the SQLite database file\n",
    "    )\n",
    "    \"Standardized SQLite storage for media analysis results.\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            db_path: str  # Absolute path to the SQLite database file\n",
    "        )\n",
    "        \"Initialize storage and create table if needed.\"\n",
    "    \n",
    "    def save(\n",
    "            self,\n",
    "            file_path: str,     # Path to the analyzed media file\n",
    "            file_hash: str,     # Hash of source file in \"algo:hexdigest\" format\n",
    "            config_hash: str,   # Hash of the analysis config\n",
    "            ranges: Optional[List[Dict[str, Any]]] = None,  # Detected temporal segments\n",
    "            metadata: Optional[Dict[str, Any]] = None        # Analysis metadata\n",
    "        ) -> None\n",
    "        \"Save or replace an analysis result (upsert by file_path + config_hash).\"\n",
    "    \n",
    "    def get_cached(\n",
    "            self,\n",
    "            file_path: str,   # Path to the media file\n",
    "            config_hash: str  # Config hash to match\n",
    "        ) -> Optional[MediaAnalysisRow]:  # Cached row or None\n",
    "        \"Retrieve a cached analysis result by file path and config hash.\"\n",
    "    \n",
    "    def list_jobs(\n",
    "            self,\n",
    "            limit: int = 100  # Maximum number of rows to return\n",
    "        ) -> List[MediaAnalysisRow]:  # List of analysis rows\n",
    "        \"List analysis jobs ordered by creation time (newest first).\"\n",
    "    \n",
    "    def verify_file(\n",
    "            self,\n",
    "            file_path: str,   # Path to the media file\n",
    "            config_hash: str  # Config hash to look up\n",
    "        ) -> Optional[bool]:  # True if file matches, False if changed, None if not found\n",
    "        \"Verify the source media file still matches its stored hash.\"\n",
    "```\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class MediaProcessingRow:\n",
    "    \"A single row from the processing_jobs table.\"\n",
    "    \n",
    "    job_id: str  # Unique job identifier\n",
    "    action: str  # Operation performed: 'convert', 'extract_segment', etc.\n",
    "    input_path: str  # Path to the source media file\n",
    "    input_hash: str  # Hash of source file in \"algo:hexdigest\" format\n",
    "    output_path: str  # Path to the produced output file\n",
    "    output_hash: str  # Hash of output file in \"algo:hexdigest\" format\n",
    "    parameters: Optional[Dict[str, Any]]  # Action-specific parameters\n",
    "    metadata: Optional[Dict[str, Any]]  # Processing metadata\n",
    "    created_at: Optional[float]  # Unix timestamp\n",
    "```\n",
    "\n",
    "```python\n",
    "class MediaProcessingStorage:\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str  # Absolute path to the SQLite database file\n",
    "    )\n",
    "    \"Standardized SQLite storage for media processing results.\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            db_path: str  # Absolute path to the SQLite database file\n",
    "        )\n",
    "        \"Initialize storage and create table if needed.\"\n",
    "    \n",
    "    def save(\n",
    "            self,\n",
    "            job_id: str,        # Unique job identifier\n",
    "            action: str,        # Operation performed: 'convert', 'extract_segment', etc.\n",
    "            input_path: str,    # Path to the source media file\n",
    "            input_hash: str,    # Hash of source file in \"algo:hexdigest\" format\n",
    "            output_path: str,   # Path to the produced output file\n",
    "            output_hash: str,   # Hash of output file in \"algo:hexdigest\" format\n",
    "            parameters: Optional[Dict[str, Any]] = None,  # Action-specific parameters\n",
    "            metadata: Optional[Dict[str, Any]] = None       # Processing metadata\n",
    "        ) -> None\n",
    "        \"Save a media processing result to the database.\"\n",
    "    \n",
    "    def get_by_job_id(\n",
    "            self,\n",
    "            job_id: str  # Job identifier to look up\n",
    "        ) -> Optional[MediaProcessingRow]:  # Row or None if not found\n",
    "        \"Retrieve a processing result by job ID.\"\n",
    "    \n",
    "    def list_jobs(\n",
    "            self,\n",
    "            limit: int = 100  # Maximum number of rows to return\n",
    "        ) -> List[MediaProcessingRow]:  # List of processing rows\n",
    "        \"List processing jobs ordered by creation time (newest first).\"\n",
    "    \n",
    "    def verify_input(\n",
    "            self,\n",
    "            job_id: str  # Job identifier to verify\n",
    "        ) -> Optional[bool]:  # True if input matches, False if changed, None if not found\n",
    "        \"Verify the source media file still matches its stored hash.\"\n",
    "    \n",
    "    def verify_output(\n",
    "            self,\n",
    "            job_id: str  # Job identifier to verify\n",
    "        ) -> Optional[bool]:  # True if output matches, False if changed, None if not found\n",
    "        \"Verify the output media file still matches its stored hash.\"\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
