{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Media Storage\n",
    "\n",
    "> Standardized SQLite storage for media analysis and processing results with content hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-showdoc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import sqlite3\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from cjm_plugin_system.utils.hashing import hash_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-row-docs",
   "metadata": {},
   "source": [
    "## MediaAnalysisRow\n",
    "\n",
    "A dataclass representing a single row in the standardized `analysis_jobs` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-row-dataclass",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class MediaAnalysisRow:\n",
    "    \"\"\"A single row from the analysis_jobs table.\"\"\"\n",
    "    file_path: str       # Path to the analyzed media file\n",
    "    file_hash: str       # Hash of source file in \"algo:hexdigest\" format\n",
    "    config_hash: str     # Hash of the analysis config used\n",
    "    ranges: Optional[List[Dict[str, Any]]] = None  # Detected temporal segments\n",
    "    metadata: Optional[Dict[str, Any]] = None       # Analysis metadata\n",
    "    created_at: Optional[float] = None               # Unix timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-row",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: file_path=/tmp/test.mp3\n",
      "File hash: sha256:aaaaaaaaaaaaa...\n",
      "Config hash: sha256:bbbbbbbbbbbbb...\n"
     ]
    }
   ],
   "source": [
    "# Test MediaAnalysisRow creation\n",
    "row = MediaAnalysisRow(\n",
    "    file_path=\"/tmp/test.mp3\",\n",
    "    file_hash=\"sha256:\" + \"a\" * 64,\n",
    "    config_hash=\"sha256:\" + \"b\" * 64,\n",
    "    ranges=[{\"start\": 0.0, \"end\": 2.5, \"label\": \"speech\"}],\n",
    "    metadata={\"segment_count\": 1}\n",
    ")\n",
    "\n",
    "print(f\"Row: file_path={row.file_path}\")\n",
    "print(f\"File hash: {row.file_hash[:20]}...\")\n",
    "print(f\"Config hash: {row.config_hash[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-storage-docs",
   "metadata": {},
   "source": [
    "## MediaAnalysisStorage\n",
    "\n",
    "Standardized SQLite storage that all media analysis plugins should use. Defines the canonical schema for the `analysis_jobs` table with file hashing for traceability and config-based caching.\n",
    "\n",
    "**Schema:**\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS analysis_jobs (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    file_path TEXT NOT NULL,\n",
    "    file_hash TEXT NOT NULL,\n",
    "    config_hash TEXT NOT NULL,\n",
    "    ranges JSON,\n",
    "    metadata JSON,\n",
    "    created_at REAL NOT NULL,\n",
    "    UNIQUE(file_path, config_hash)\n",
    ");\n",
    "```\n",
    "\n",
    "The `UNIQUE(file_path, config_hash)` constraint enables result caching â€” re-running the same file with the same config replaces the previous result. Different configs for the same file are stored separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-storage-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MediaAnalysisStorage:\n",
    "    \"\"\"Standardized SQLite storage for media analysis results.\"\"\"\n",
    "\n",
    "    SCHEMA = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS analysis_jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            file_path TEXT NOT NULL,\n",
    "            file_hash TEXT NOT NULL,\n",
    "            config_hash TEXT NOT NULL,\n",
    "            ranges JSON,\n",
    "            metadata JSON,\n",
    "            created_at REAL NOT NULL,\n",
    "            UNIQUE(file_path, config_hash)\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    INDEX = \"CREATE INDEX IF NOT EXISTS idx_analysis_file_path ON analysis_jobs(file_path);\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str  # Absolute path to the SQLite database file\n",
    "    ):\n",
    "        \"\"\"Initialize storage and create table if needed.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            con.execute(self.SCHEMA)\n",
    "            con.execute(self.INDEX)\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        file_path: str,     # Path to the analyzed media file\n",
    "        file_hash: str,     # Hash of source file in \"algo:hexdigest\" format\n",
    "        config_hash: str,   # Hash of the analysis config\n",
    "        ranges: Optional[List[Dict[str, Any]]] = None,  # Detected temporal segments\n",
    "        metadata: Optional[Dict[str, Any]] = None        # Analysis metadata\n",
    "    ) -> None:\n",
    "        \"\"\"Save or replace an analysis result (upsert by file_path + config_hash).\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            con.execute(\n",
    "                \"\"\"INSERT OR REPLACE INTO analysis_jobs\n",
    "                   (file_path, file_hash, config_hash, ranges, metadata, created_at)\n",
    "                   VALUES (?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                (\n",
    "                    file_path,\n",
    "                    file_hash,\n",
    "                    config_hash,\n",
    "                    json.dumps(ranges) if ranges else None,\n",
    "                    json.dumps(metadata) if metadata else None,\n",
    "                    time.time()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_cached(\n",
    "        self,\n",
    "        file_path: str,   # Path to the media file\n",
    "        config_hash: str  # Config hash to match\n",
    "    ) -> Optional[MediaAnalysisRow]:  # Cached row or None\n",
    "        \"\"\"Retrieve a cached analysis result by file path and config hash.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            cur = con.execute(\n",
    "                \"\"\"SELECT file_path, file_hash, config_hash, ranges, metadata, created_at\n",
    "                   FROM analysis_jobs WHERE file_path = ? AND config_hash = ?\"\"\",\n",
    "                (file_path, config_hash)\n",
    "            )\n",
    "            row = cur.fetchone()\n",
    "            if not row:\n",
    "                return None\n",
    "            return MediaAnalysisRow(\n",
    "                file_path=row[0],\n",
    "                file_hash=row[1],\n",
    "                config_hash=row[2],\n",
    "                ranges=json.loads(row[3]) if row[3] else None,\n",
    "                metadata=json.loads(row[4]) if row[4] else None,\n",
    "                created_at=row[5]\n",
    "            )\n",
    "\n",
    "    def list_jobs(\n",
    "        self,\n",
    "        limit: int = 100  # Maximum number of rows to return\n",
    "    ) -> List[MediaAnalysisRow]:  # List of analysis rows\n",
    "        \"\"\"List analysis jobs ordered by creation time (newest first).\"\"\"\n",
    "        results = []\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            cur = con.execute(\n",
    "                \"\"\"SELECT file_path, file_hash, config_hash, ranges, metadata, created_at\n",
    "                   FROM analysis_jobs ORDER BY created_at DESC LIMIT ?\"\"\",\n",
    "                (limit,)\n",
    "            )\n",
    "            for row in cur:\n",
    "                results.append(MediaAnalysisRow(\n",
    "                    file_path=row[0],\n",
    "                    file_hash=row[1],\n",
    "                    config_hash=row[2],\n",
    "                    ranges=json.loads(row[3]) if row[3] else None,\n",
    "                    metadata=json.loads(row[4]) if row[4] else None,\n",
    "                    created_at=row[5]\n",
    "                ))\n",
    "        return results\n",
    "\n",
    "    def verify_file(\n",
    "        self,\n",
    "        file_path: str,   # Path to the media file\n",
    "        config_hash: str  # Config hash to look up\n",
    "    ) -> Optional[bool]:  # True if file matches, False if changed, None if not found\n",
    "        \"\"\"Verify the source media file still matches its stored hash.\"\"\"\n",
    "        row = self.get_cached(file_path, config_hash)\n",
    "        if not row:\n",
    "            return None\n",
    "        current_hash = hash_file(row.file_path)\n",
    "        return current_hash == row.file_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-testing-header",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage initialized at: /tmp/tmp04637ejq.db\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create storage with temp database\n",
    "tmp_db = tempfile.NamedTemporaryFile(suffix=\".db\", delete=False)\n",
    "storage = MediaAnalysisStorage(tmp_db.name)\n",
    "\n",
    "print(f\"Storage initialized at: {tmp_db.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis result\n"
     ]
    }
   ],
   "source": [
    "# Save an analysis result\n",
    "storage.save(\n",
    "    file_path=\"/tmp/test_audio.mp3\",\n",
    "    file_hash=\"sha256:\" + \"a\" * 64,\n",
    "    config_hash=\"sha256:\" + \"c\" * 64,\n",
    "    ranges=[\n",
    "        {\"start\": 0.0, \"end\": 2.5, \"label\": \"speech\", \"confidence\": 0.98},\n",
    "        {\"start\": 4.0, \"end\": 8.5, \"label\": \"speech\", \"confidence\": 0.95}\n",
    "    ],\n",
    "    metadata={\"segment_count\": 2, \"total_speech\": 7.0}\n",
    ")\n",
    "\n",
    "print(\"Saved analysis result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: /tmp/test_audio.mp3\n",
      "Ranges: 2 segments\n",
      "File hash: sha256:aaaaaaaaaaaaa...\n",
      "Cache miss for different config: OK\n"
     ]
    }
   ],
   "source": [
    "# Retrieve cached result\n",
    "cached = storage.get_cached(\"/tmp/test_audio.mp3\", \"sha256:\" + \"c\" * 64)\n",
    "assert cached is not None\n",
    "assert cached.file_path == \"/tmp/test_audio.mp3\"\n",
    "assert len(cached.ranges) == 2\n",
    "assert cached.metadata[\"segment_count\"] == 2\n",
    "assert cached.created_at is not None\n",
    "\n",
    "print(f\"Cached: {cached.file_path}\")\n",
    "print(f\"Ranges: {len(cached.ranges)} segments\")\n",
    "print(f\"File hash: {cached.file_hash[:20]}...\")\n",
    "\n",
    "# Missing config returns None\n",
    "missing = storage.get_cached(\"/tmp/test_audio.mp3\", \"sha256:\" + \"d\" * 64)\n",
    "assert missing is None\n",
    "print(\"Cache miss for different config: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-upsert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert replaced existing row: OK\n"
     ]
    }
   ],
   "source": [
    "# Save with same file+config replaces (upsert)\n",
    "storage.save(\n",
    "    file_path=\"/tmp/test_audio.mp3\",\n",
    "    file_hash=\"sha256:\" + \"a\" * 64,\n",
    "    config_hash=\"sha256:\" + \"c\" * 64,\n",
    "    ranges=[{\"start\": 0.0, \"end\": 3.0, \"label\": \"speech\"}],\n",
    "    metadata={\"segment_count\": 1, \"total_speech\": 3.0}\n",
    ")\n",
    "\n",
    "updated = storage.get_cached(\"/tmp/test_audio.mp3\", \"sha256:\" + \"c\" * 64)\n",
    "assert len(updated.ranges) == 1  # Updated to 1 range\n",
    "assert updated.metadata[\"segment_count\"] == 1\n",
    "\n",
    "# Only 1 row total (replaced, not appended)\n",
    "all_jobs = storage.list_jobs()\n",
    "assert len(all_jobs) == 1\n",
    "\n",
    "print(\"Upsert replaced existing row: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-multi-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two configs for same file: 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Different config for same file creates separate row\n",
    "storage.save(\n",
    "    file_path=\"/tmp/test_audio.mp3\",\n",
    "    file_hash=\"sha256:\" + \"a\" * 64,\n",
    "    config_hash=\"sha256:\" + \"e\" * 64,  # Different config\n",
    "    ranges=[{\"start\": 0.5, \"end\": 2.0, \"label\": \"speech\"}],\n",
    "    metadata={\"segment_count\": 1}\n",
    ")\n",
    "\n",
    "all_jobs = storage.list_jobs()\n",
    "assert len(all_jobs) == 2\n",
    "\n",
    "print(f\"Two configs for same file: {len(all_jobs)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "os.unlink(tmp_db.name)\n",
    "print(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "astpgza0xz7",
   "metadata": {},
   "source": [
    "## MediaProcessingRow\n",
    "\n",
    "A dataclass representing a single row in the standardized `processing_jobs` table. Tracks input/output file pairs with hashes for full traceability of media transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ktl2o719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class MediaProcessingRow:\n",
    "    \"\"\"A single row from the processing_jobs table.\"\"\"\n",
    "    job_id: str          # Unique job identifier\n",
    "    action: str          # Operation performed: 'convert', 'extract_segment', etc.\n",
    "    input_path: str      # Path to the source media file\n",
    "    input_hash: str      # Hash of source file in \"algo:hexdigest\" format\n",
    "    output_path: str     # Path to the produced output file\n",
    "    output_hash: str     # Hash of output file in \"algo:hexdigest\" format\n",
    "    parameters: Optional[Dict[str, Any]] = None  # Action-specific parameters\n",
    "    metadata: Optional[Dict[str, Any]] = None     # Processing metadata\n",
    "    created_at: Optional[float] = None             # Unix timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9q29dfcv5kk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: job_id=job_conv_001, action=convert\n",
      "Input: /tmp/source.mkv -> Output: /tmp/output.mp4\n"
     ]
    }
   ],
   "source": [
    "# Test MediaProcessingRow creation\n",
    "proc_row = MediaProcessingRow(\n",
    "    job_id=\"job_conv_001\",\n",
    "    action=\"convert\",\n",
    "    input_path=\"/tmp/source.mkv\",\n",
    "    input_hash=\"sha256:\" + \"a\" * 64,\n",
    "    output_path=\"/tmp/output.mp4\",\n",
    "    output_hash=\"sha256:\" + \"b\" * 64,\n",
    "    parameters={\"output_format\": \"mp4\", \"codec\": \"h264\"}\n",
    ")\n",
    "\n",
    "print(f\"Row: job_id={proc_row.job_id}, action={proc_row.action}\")\n",
    "print(f\"Input: {proc_row.input_path} -> Output: {proc_row.output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dz9cw2lvg2s",
   "metadata": {},
   "source": [
    "## MediaProcessingStorage\n",
    "\n",
    "Standardized SQLite storage that all media processing plugins should use. Defines the canonical schema for the `processing_jobs` table, tracking input/output file pairs with content hashes.\n",
    "\n",
    "**Schema:**\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS processing_jobs (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    job_id TEXT UNIQUE NOT NULL,\n",
    "    action TEXT NOT NULL,\n",
    "    input_path TEXT NOT NULL,\n",
    "    input_hash TEXT NOT NULL,\n",
    "    output_path TEXT NOT NULL,\n",
    "    output_hash TEXT NOT NULL,\n",
    "    parameters JSON,\n",
    "    metadata JSON,\n",
    "    created_at REAL NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "Both `input_hash` and `output_hash` use the self-describing `\"algo:hexdigest\"` format, enabling verification of both source integrity (\"is this the same file we converted?\") and output integrity (\"has the output been modified since conversion?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wdlaqv0ay9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MediaProcessingStorage:\n",
    "    \"\"\"Standardized SQLite storage for media processing results.\"\"\"\n",
    "\n",
    "    SCHEMA = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS processing_jobs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_id TEXT UNIQUE NOT NULL,\n",
    "            action TEXT NOT NULL,\n",
    "            input_path TEXT NOT NULL,\n",
    "            input_hash TEXT NOT NULL,\n",
    "            output_path TEXT NOT NULL,\n",
    "            output_hash TEXT NOT NULL,\n",
    "            parameters JSON,\n",
    "            metadata JSON,\n",
    "            created_at REAL NOT NULL\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    INDEX = \"CREATE INDEX IF NOT EXISTS idx_processing_job_id ON processing_jobs(job_id);\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str  # Absolute path to the SQLite database file\n",
    "    ):\n",
    "        \"\"\"Initialize storage and create table if needed.\"\"\"\n",
    "        self.db_path = db_path\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            con.execute(self.SCHEMA)\n",
    "            con.execute(self.INDEX)\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        job_id: str,        # Unique job identifier\n",
    "        action: str,        # Operation performed: 'convert', 'extract_segment', etc.\n",
    "        input_path: str,    # Path to the source media file\n",
    "        input_hash: str,    # Hash of source file in \"algo:hexdigest\" format\n",
    "        output_path: str,   # Path to the produced output file\n",
    "        output_hash: str,   # Hash of output file in \"algo:hexdigest\" format\n",
    "        parameters: Optional[Dict[str, Any]] = None,  # Action-specific parameters\n",
    "        metadata: Optional[Dict[str, Any]] = None       # Processing metadata\n",
    "    ) -> None:\n",
    "        \"\"\"Save a media processing result to the database.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            con.execute(\n",
    "                \"\"\"INSERT INTO processing_jobs\n",
    "                   (job_id, action, input_path, input_hash, output_path, output_hash,\n",
    "                    parameters, metadata, created_at)\n",
    "                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "                (\n",
    "                    job_id,\n",
    "                    action,\n",
    "                    input_path,\n",
    "                    input_hash,\n",
    "                    output_path,\n",
    "                    output_hash,\n",
    "                    json.dumps(parameters) if parameters else None,\n",
    "                    json.dumps(metadata) if metadata else None,\n",
    "                    time.time()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_by_job_id(\n",
    "        self,\n",
    "        job_id: str  # Job identifier to look up\n",
    "    ) -> Optional[MediaProcessingRow]:  # Row or None if not found\n",
    "        \"\"\"Retrieve a processing result by job ID.\"\"\"\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            cur = con.execute(\n",
    "                \"\"\"SELECT job_id, action, input_path, input_hash, output_path, output_hash,\n",
    "                          parameters, metadata, created_at\n",
    "                   FROM processing_jobs WHERE job_id = ?\"\"\",\n",
    "                (job_id,)\n",
    "            )\n",
    "            row = cur.fetchone()\n",
    "            if not row:\n",
    "                return None\n",
    "            return MediaProcessingRow(\n",
    "                job_id=row[0],\n",
    "                action=row[1],\n",
    "                input_path=row[2],\n",
    "                input_hash=row[3],\n",
    "                output_path=row[4],\n",
    "                output_hash=row[5],\n",
    "                parameters=json.loads(row[6]) if row[6] else None,\n",
    "                metadata=json.loads(row[7]) if row[7] else None,\n",
    "                created_at=row[8]\n",
    "            )\n",
    "\n",
    "    def list_jobs(\n",
    "        self,\n",
    "        limit: int = 100  # Maximum number of rows to return\n",
    "    ) -> List[MediaProcessingRow]:  # List of processing rows\n",
    "        \"\"\"List processing jobs ordered by creation time (newest first).\"\"\"\n",
    "        results = []\n",
    "        with sqlite3.connect(self.db_path) as con:\n",
    "            cur = con.execute(\n",
    "                \"\"\"SELECT job_id, action, input_path, input_hash, output_path, output_hash,\n",
    "                          parameters, metadata, created_at\n",
    "                   FROM processing_jobs ORDER BY created_at DESC LIMIT ?\"\"\",\n",
    "                (limit,)\n",
    "            )\n",
    "            for row in cur:\n",
    "                results.append(MediaProcessingRow(\n",
    "                    job_id=row[0],\n",
    "                    action=row[1],\n",
    "                    input_path=row[2],\n",
    "                    input_hash=row[3],\n",
    "                    output_path=row[4],\n",
    "                    output_hash=row[5],\n",
    "                    parameters=json.loads(row[6]) if row[6] else None,\n",
    "                    metadata=json.loads(row[7]) if row[7] else None,\n",
    "                    created_at=row[8]\n",
    "                ))\n",
    "        return results\n",
    "\n",
    "    def verify_input(\n",
    "        self,\n",
    "        job_id: str  # Job identifier to verify\n",
    "    ) -> Optional[bool]:  # True if input matches, False if changed, None if not found\n",
    "        \"\"\"Verify the source media file still matches its stored hash.\"\"\"\n",
    "        row = self.get_by_job_id(job_id)\n",
    "        if not row:\n",
    "            return None\n",
    "        current_hash = hash_file(row.input_path)\n",
    "        return current_hash == row.input_hash\n",
    "\n",
    "    def verify_output(\n",
    "        self,\n",
    "        job_id: str  # Job identifier to verify\n",
    "    ) -> Optional[bool]:  # True if output matches, False if changed, None if not found\n",
    "        \"\"\"Verify the output media file still matches its stored hash.\"\"\"\n",
    "        row = self.get_by_job_id(job_id)\n",
    "        if not row:\n",
    "            return None\n",
    "        current_hash = hash_file(row.output_path)\n",
    "        return current_hash == row.output_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dzgap4z6tx",
   "metadata": {},
   "source": [
    "### Testing MediaProcessingStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pqf6b6jj0rf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing storage initialized at: /tmp/tmp6cy8gfmq.db\n"
     ]
    }
   ],
   "source": [
    "# Create processing storage with temp database\n",
    "tmp_db2 = tempfile.NamedTemporaryFile(suffix=\".db\", delete=False)\n",
    "proc_storage = MediaProcessingStorage(tmp_db2.name)\n",
    "\n",
    "print(f\"Processing storage initialized at: {tmp_db2.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j14q54fgo1s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved conversion job\n"
     ]
    }
   ],
   "source": [
    "# Save a conversion job\n",
    "proc_storage.save(\n",
    "    job_id=\"job_conv_001\",\n",
    "    action=\"convert\",\n",
    "    input_path=\"/tmp/source.mkv\",\n",
    "    input_hash=\"sha256:\" + \"a\" * 64,\n",
    "    output_path=\"/tmp/output.mp4\",\n",
    "    output_hash=\"sha256:\" + \"b\" * 64,\n",
    "    parameters={\"output_format\": \"mp4\", \"codec\": \"h264\"},\n",
    "    metadata={\"duration\": 120.5}\n",
    ")\n",
    "\n",
    "print(\"Saved conversion job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u2jgf6c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: job_conv_001 (convert)\n",
      "Input: /tmp/source.mkv (sha256:aaaaaaaaaaaaa...)\n",
      "Output: /tmp/output.mp4 (sha256:bbbbbbbbbbbbb...)\n",
      "get_by_job_id returns None for missing job: OK\n"
     ]
    }
   ],
   "source": [
    "# Retrieve by job ID\n",
    "row = proc_storage.get_by_job_id(\"job_conv_001\")\n",
    "assert row is not None\n",
    "assert row.job_id == \"job_conv_001\"\n",
    "assert row.action == \"convert\"\n",
    "assert row.input_path == \"/tmp/source.mkv\"\n",
    "assert row.output_path == \"/tmp/output.mp4\"\n",
    "assert row.parameters[\"output_format\"] == \"mp4\"\n",
    "assert row.created_at is not None\n",
    "\n",
    "print(f\"Retrieved: {row.job_id} ({row.action})\")\n",
    "print(f\"Input: {row.input_path} ({row.input_hash[:20]}...)\")\n",
    "print(f\"Output: {row.output_path} ({row.output_hash[:20]}...)\")\n",
    "\n",
    "# Missing job returns None\n",
    "assert proc_storage.get_by_job_id(\"nonexistent\") is None\n",
    "print(\"get_by_job_id returns None for missing job: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0r2cx9i0v0j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_jobs returned 2 rows: [('job_ext_001', 'extract_segment'), ('job_conv_001', 'convert')]\n"
     ]
    }
   ],
   "source": [
    "# Save an extract_segment job and test list_jobs\n",
    "proc_storage.save(\n",
    "    job_id=\"job_ext_001\",\n",
    "    action=\"extract_segment\",\n",
    "    input_path=\"/tmp/source.mkv\",\n",
    "    input_hash=\"sha256:\" + \"a\" * 64,\n",
    "    output_path=\"/tmp/segment_10-20.wav\",\n",
    "    output_hash=\"sha256:\" + \"c\" * 64,\n",
    "    parameters={\"start\": 10.0, \"end\": 20.0}\n",
    ")\n",
    "\n",
    "jobs = proc_storage.list_jobs()\n",
    "assert len(jobs) == 2\n",
    "assert jobs[0].job_id == \"job_ext_001\"  # Newest first\n",
    "assert jobs[0].action == \"extract_segment\"\n",
    "\n",
    "print(f\"list_jobs returned {len(jobs)} rows: {[(j.job_id, j.action) for j in jobs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rxbow4dza0o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing storage cleanup complete\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "os.unlink(tmp_db2.name)\n",
    "print(\"Processing storage cleanup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-nbdev-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
